{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'pen']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'i have a pen'\n",
    "a1 = a.split(' ')\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have a pen']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = a.split(' ', 1)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i ', ' a pen']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3 = a.split('have', 1)\n",
    "a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标记化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_english_text(text):\n",
    "    tokenized_text = []\n",
    "    for data in text:\n",
    "        tokenized_data = []\n",
    "        for s in data.split('.'):\n",
    "            for s1 in s.split('?'):\n",
    "                for s2 in s1.split('!'):\n",
    "                    for s3 in s2.split(','):\n",
    "                        # if s4!='':  这一步是去除空字符''。注意与' ' 的区别。\n",
    "                        tokenized_data.extend(s4 for s4 in s3.split(' ') if s4 != '')\n",
    "        tokenized_text.append(tokenized_data)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am', 'a', 'boy', 'i', 'am', 'a', 'boy', 'i', 'am', 'a', 'boy', 'i'],\n",
       " ['god', 'is', 'a', 'girl'],\n",
       " ['i', 'love', 'you']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['i am a boy?i am a boy ! i am a boy,i', 'god is a girl', 'i love you!']\n",
    "result = tokenize_english_text(a)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机械分词方法\n",
    "- 统计分词方法\n",
    "- 两种结合起来的分词方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 机械分词方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机械分词方法又叫做基于规则的分词方法：这种分词方法按照一定的规则将待处理的字符串与一个词表词典中的词进行逐一匹配，若在词典中找到某个字符串，则切分，否则不切分。机械分词方法按照匹配规则的方式，又可以分为：正向最大匹配法，逆向最大匹配法和双向匹配法三种。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 正向最大匹配法 Maximum Match Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我们是共产主义的接班人'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试文本\n",
    "text = '我们是共产主义的接班人'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('我们', '是', '共产主义', '的', '接班', '人', '你', '我', '社会', '主义')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词典\n",
    "dic = ('我们', '是', '共产主义', '的', '接班', '人', '你', '我', '社会', '主义')\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始最长词长度为 0\n",
    "max_len_word0 = 0\n",
    "for key in dic:\n",
    "    # 若当前词长度大于 max_len_word，则将 len(key)值赋值给 max_len_word\n",
    "    if len(key) > max_len_word0:\n",
    "        max_len_word0 = len(key)\n",
    "\n",
    "max_len_word0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用 【 我们是共 】 进行匹配\n",
      "用 【 我们是 】 进行匹配\n",
      "用 【 我们 】 进行匹配\n",
      "【我们】 匹配成功，添加进 words 当中\n",
      "--------------------------------------------------\n",
      "用 【 是共 】 进行匹配\n",
      "用 【 是共产主 】 进行匹配\n",
      "用 【 是共产 】 进行匹配\n",
      "用 【 是共 】 进行匹配\n",
      "用 【 是 】 进行匹配\n",
      "【是】 匹配成功，添加进 words 当中\n",
      "--------------------------------------------------\n",
      "用 【 共产主义 】 进行匹配\n",
      "【共产主义】 匹配成功，添加进 words 当中\n",
      "--------------------------------------------------\n",
      "用 【 的接班人 】 进行匹配\n",
      "用 【 的接班 】 进行匹配\n",
      "用 【 的接 】 进行匹配\n",
      "用 【 的接班人 】 进行匹配\n",
      "用 【 的接班 】 进行匹配\n",
      "用 【 的接 】 进行匹配\n",
      "用 【 的 】 进行匹配\n",
      "【的】 匹配成功，添加进 words 当中\n",
      "--------------------------------------------------\n",
      "用 【 接班人 】 进行匹配\n",
      "用 【 接班人 】 进行匹配\n",
      "用 【 接班 】 进行匹配\n",
      "【接班】 匹配成功，添加进 words 当中\n",
      "--------------------------------------------------\n",
      "用 【 人 】 进行匹配\n",
      "【人】 匹配成功，添加进 words 当中\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sent = text\n",
    "words = []   \n",
    "max_len_word = max_len_word0\n",
    "# 判断 text 的长度是否大于 0，如果大于 0 则进行下面的循环\n",
    "while len(sent) > 0:\n",
    "    # 初始化想要取的字符串长度\n",
    "    # 按照最长词长度初始化\n",
    "    word_len = max_len_word\n",
    "    # 对每个字符串可能会有(max_len_word)次循环\n",
    "    for i in range(0, max_len_word):\n",
    "        # 令 word 等于 text 的前 word_len 个字符\n",
    "        word = sent[0:word_len]\n",
    "        # 为了便于观察过程，我们打印一下当前分割结果\n",
    "        print('用 【', word, '】 进行匹配')\n",
    "        # 判断 word 是否在词典 dic 当中\n",
    "        # 如果不在词典当中\n",
    "        if word not in dic:\n",
    "            # 则以 word_len - 1\n",
    "            word_len -= 1\n",
    "            # 清空 word\n",
    "            word = []\n",
    "        # 如果 word 在词典当中\n",
    "        else:\n",
    "            # 更新 text 串起始位置\n",
    "            sent = sent[word_len:]\n",
    "            # 为了方便观察过程，我们打印一下当前结果\n",
    "            print('【{}】 匹配成功，添加进 words 当中'.format(word))\n",
    "            print('-'*50)\n",
    "            # 把匹配成功的word添加进上面创建好的words当中\n",
    "            words.append(word)\n",
    "            # 清空word\n",
    "            word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们', '是', '共产主义', '的', '接班', '人']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. 统计分词方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 语料统计方法\n",
    "- 序列标注方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 语料统计方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于语料统计方法可以这样理解：我们已经有一个由很多个文本组成的的语料库 D ，假设现在对一个句子【我有一个苹果】进行分词。其中两个相连的字 【苹】【果】在不同的文本中连续出现的次数越多，就说明这两个相连字很可能构成一个词【苹果】。与此同时 【个】【苹】 这两个相连的词在别的文本中连续出现的次数很少，就说明这两个相连的字不太可能构成一个词【个苹】。所以，我们就可以利用这个统计规则来反应字与字成词的可信度。当字连续组合的概率高过一个临界值时，就认为该组合构成了一个词语。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 序列标注方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列标注方法则将中文分词看做是一个序列标注问题。首先，规定每个字在一个词语当中有着 4 个不同的位置，词首 B，词中 M，词尾 E，单字成词 S。我们通过给一句话中的每个字标记上述的属性，最后通过标注来确定分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我| 来到| 北京| 清华| 清华大学| 华大| 大学'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '我来到北京清华大学'\n",
    "seg_list = jieba.cut(string, cut_all=True)\n",
    "'| '.join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我|来到|北京|清华大学'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我|来到|北京|清华|华大|大学|清华大学'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search(string)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "长文本作分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'市场有很多机遇但同时也充满杀机，野蛮生长和快速发展中如何慢慢稳住底盘，驾驭风险，保持起伏冲撞在合理的范围，特别是新兴行业，领军企业更得有胸怀和大局，需要在竞争中保持张弛有度，促成行业建立同盟和百花争艳的健康持续的多赢局面，而非最后比的是谁狠，比的是谁更有底线，劣币驱逐良币，最终谁都逃不了要还的。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '市场有很多机遇但同时也充满杀机，野蛮生长和快速发展中如何慢慢稳住底盘，驾驭风险，保持起伏冲撞在合理的范围，特别是新兴行业，领军企业更得有胸怀和大局，需要在竞争中保持张弛有度，促成行业建立同盟和百花争艳的健康持续的多赢局面，而非最后比的是谁狠，比的是谁更有底线，劣币驱逐良币，最终谁都逃不了要还的。'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'市场|有|很多|机遇|但|同时|也|充满|杀机|，|野蛮|生长|和|快速|发展|中|如何|慢慢|稳住|底盘|，|驾驭|风险|，|保持|起伏|冲撞|在|合理|的|范围|，|特别|是|新兴|行业|，|领军|企业|更得|有|胸怀|和|大局|，|需要|在|竞争|中|保持|张弛|有度|，|促成|行业|建立|同盟|和|百花争艳|的|健康|持续|的|多|赢|局面|，|而|非|最后|比|的|是|谁|狠|，|比|的|是|谁|更|有|底线|，|劣币|驱逐|良币|，|最终|谁|都|逃不了|要|还|的|。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jieba.cut(text, cut_all=False)\n",
    "'|'.join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天天气|不错'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '今天天气不错'\n",
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天|天气|不错'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('今天', '天气'), True)\n",
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天|天气|不错'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.del_word('今天天气')\n",
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调高词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'台|中'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '台中'\n",
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'台中'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.add_word('台中')\n",
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 简单的过滤器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('的', '地', '得')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立停用词表\n",
    "stopwords = ('的', '地', '得')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我喜欢的和你讨厌地以及最不想要得'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '我喜欢的和你讨厌地以及最不想要得'\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我|喜欢|的|和|你|讨厌|地|以及|最|不|想要|得'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "'|'.join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '喜欢', '和', '你', '讨厌', '以及', '最', '不', '想要']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "seg_list = jieba.cut(string, cut_all=False)\n",
    "for word in seg_list:\n",
    "    if word not in stopwords:\n",
    "        a.append(word)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
